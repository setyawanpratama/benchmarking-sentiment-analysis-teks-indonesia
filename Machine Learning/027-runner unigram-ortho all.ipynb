{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import time\n",
    "import warnings\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "from featureExtraction.combination.ortho_tf import *\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "from classifier.knn.knn import *\n",
    "from classifier.svm.svm import *\n",
    "from featureExtraction.tf.tf import *\n",
    "from classifier.naiveBayes.naiveBayes import *\n",
    "from classifier.randomForest.randomForest import *\n",
    "from classifier.decisionTree.decisionTree import *\n",
    "from classifier.logisticRegression.logisticRegression import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_clean = os.listdir(\"../Dataset/Clean\")\n",
    "files_templated = os.listdir(\"../Dataset/Raw\")\n",
    "\n",
    "clean = [x for x in files_clean if (x.lower().endswith(\".csv\"))]\n",
    "clean.sort()\n",
    "\n",
    "DATA_CLEAN = [\"../Dataset/Clean/\" + x for x in files_clean if (x.lower().endswith(\".csv\"))]\n",
    "DATA_CLEAN.sort()\n",
    "len_clean = len(DATA_CLEAN)\n",
    "\n",
    "templated = [x for x in files_templated if (x.lower().endswith(\".csv\"))]\n",
    "templated.sort()\n",
    "\n",
    "DATA_TEMPLATED = [\"../Dataset/Raw/\" + x for x in files_templated if (x.lower().endswith(\".csv\"))]\n",
    "DATA_TEMPLATED.sort()\n",
    "len_templated = len(DATA_TEMPLATED)\n",
    "\n",
    "range_len = max(len_clean, len_templated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 1/13 (templated_posneg_Dataset_C_HT_4000.csv)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_37293/1824350573.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# prepare the cross-validation procedure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0macc_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogRes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_fe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'f1_micro'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mf1_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogRes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_fe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'f1_macro'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mcl_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m     cv_results = cross_validate(estimator=estimator, X=X, y=y, groups=groups,\n\u001b[0m\u001b[1;32m    446\u001b[0m                                 \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m                                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    248\u001b[0m     parallel = Parallel(n_jobs=n_jobs, verbose=verbose,\n\u001b[1;32m    249\u001b[0m                         pre_dispatch=pre_dispatch)\n\u001b[0;32m--> 250\u001b[0;31m     results = parallel(\n\u001b[0m\u001b[1;32m    251\u001b[0m         delayed(_fit_and_score)(\n\u001b[1;32m    252\u001b[0m             \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    931\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    437\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "count = 1\n",
    "fail = 0\n",
    "\n",
    "f1_list = []\n",
    "acc_list = []\n",
    "\n",
    "for i in range(range_len):\n",
    "    try:\n",
    "        print(\"Running {}/{} ({})\".format(count, len_clean, clean[count-1]))\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Feature Extraction\n",
    "        result_fe, label, feat_name = ortho_tf(clean[i])\n",
    "        fe_time = time.time()\n",
    "\n",
    "        # prepare classifier\n",
    "        cv = KFold(n_splits=20, random_state=1, shuffle=True)\n",
    "        logRes = LogisticRegression(random_state=1, max_iter=10000)\n",
    "        \n",
    "        # prepare the cross-validation procedure\n",
    "        acc_score = cross_val_score(logRes, result_fe, label, scoring='f1_micro', cv=cv, n_jobs=-1)\n",
    "        f1_score = cross_val_score(logRes, result_fe, label, scoring='f1_macro', cv=cv, n_jobs=-1)\n",
    "        cl_time = time.time()\n",
    "\n",
    "        print('Accuracy: {} ({:.5f})'.format(np.mean(acc_score), np.std(acc_score)))\n",
    "        print('F1-Score: {} ({:.5f})'.format(np.mean(f1_score), np.std(f1_score)))\n",
    "        print('FE Time: {} seconds'.format(fe_time - start_time))\n",
    "        print('CL time: {} seconds'.format(cl_time - fe_time))\n",
    "        \n",
    "        f1_list.append(np.mean(f1_score))\n",
    "        acc_list.append(np.mean(acc_score))\n",
    "        count += 1\n",
    "    except Exception as e:\n",
    "        count += 1\n",
    "        fail += 1\n",
    "        print(i, \": \", e)\n",
    "        pass\n",
    "    print(\"================================================\")\n",
    "\n",
    "print(\"================================================\")\n",
    "print(\"Overall Accuracy = {} ({})\".format(sum(acc_list)/len(acc_list), np.std(acc_list)))\n",
    "print(\"Overall F1-Score = {} ({})\".format(sum(f1_list)/len(f1_list), np.std(f1_list)))\n",
    "print(\"Failed = {}/{}\".format(fail, len_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "fail = 0\n",
    "\n",
    "f1_list = []\n",
    "acc_list = []\n",
    "\n",
    "for i in range(range_len):\n",
    "    try:\n",
    "        print(\"Running {}/{} ({})\".format(count, len_clean, clean[count-1]))\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Feature Extraction\n",
    "        result_fe, label, feat_name = ortho_tf(clean[i])\n",
    "        fe_time = time.time()\n",
    "\n",
    "        # classification\n",
    "        acc_score, f1_score = decision_tree(result_fe, label)\n",
    "        cl_time = time.time()\n",
    "\n",
    "        print('Accuracy: {} ({:.5f})'.format(np.mean(acc_score), np.std(acc_score)))\n",
    "        print('F1-Score: {} ({:.5f})'.format(np.mean(f1_score), np.std(f1_score)))\n",
    "        print('FE Time: {} seconds'.format(fe_time - start_time))\n",
    "        print('CL time: {} seconds'.format(cl_time - fe_time))\n",
    "        \n",
    "        f1_list.append(np.mean(f1_score))\n",
    "        acc_list.append(np.mean(acc_score))\n",
    "        count += 1\n",
    "    except Exception as e:\n",
    "        count += 1\n",
    "        fail += 1\n",
    "        print(i, \": \", e)\n",
    "        pass\n",
    "    print(\"================================================\")\n",
    "\n",
    "print(\"================================================\")\n",
    "print(\"Overall Accuracy = {} ({})\".format(sum(acc_list)/len(acc_list), np.std(acc_list)))\n",
    "print(\"Overall F1-Score = {} ({})\".format(sum(f1_list)/len(f1_list), np.std(f1_list)))\n",
    "print(\"Failed = {}/{}\".format(fail, len_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "fail = 0\n",
    "\n",
    "f1_list = []\n",
    "acc_list = []\n",
    "\n",
    "for i in range(range_len):\n",
    "    try:\n",
    "        print(\"Running {}/{} ({})\".format(count, len_clean, clean[count-1]))\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Feature Extraction\n",
    "        result_fe, label, feat_name = ortho_tf(clean[i])\n",
    "        fe_time = time.time()\n",
    "\n",
    "        # classification\n",
    "        acc_score, f1_score = knn(result_fe, label)\n",
    "        cl_time = time.time()\n",
    "\n",
    "        print('Accuracy: {} ({:.5f})'.format(np.mean(acc_score), np.std(acc_score)))\n",
    "        print('F1-Score: {} ({:.5f})'.format(np.mean(f1_score), np.std(f1_score)))\n",
    "        print('FE Time: {} seconds'.format(fe_time - start_time))\n",
    "        print('CL time: {} seconds'.format(cl_time - fe_time))\n",
    "        \n",
    "        f1_list.append(np.mean(f1_score))\n",
    "        acc_list.append(np.mean(acc_score))\n",
    "        count += 1\n",
    "    except Exception as e:\n",
    "        count += 1\n",
    "        fail += 1\n",
    "        print(i, \": \", e)\n",
    "        pass\n",
    "    print(\"================================================\")\n",
    "\n",
    "print(\"================================================\")\n",
    "print(\"Overall Accuracy = {} ({})\".format(sum(acc_list)/len(acc_list), np.std(acc_list)))\n",
    "print(\"Overall F1-Score = {} ({})\".format(sum(f1_list)/len(f1_list), np.std(f1_list)))\n",
    "print(\"Failed = {}/{}\".format(fail, len_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "fail = 0\n",
    "\n",
    "f1_list = []\n",
    "acc_list = []\n",
    "\n",
    "for i in range(range_len):\n",
    "    try:\n",
    "        print(\"Running {}/{} ({})\".format(count, len_clean, clean[count-1]))\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Feature Extraction\n",
    "        result_fe, label, feat_name = ortho_tf(clean[i])\n",
    "        fe_time = time.time()\n",
    "\n",
    "        # classification\n",
    "        acc_score, f1_score = naiveBayes(result_fe, label)\n",
    "        cl_time = time.time()\n",
    "\n",
    "        print('Accuracy: {} ({:.5f})'.format(np.mean(acc_score), np.std(acc_score)))\n",
    "        print('F1-Score: {} ({:.5f})'.format(np.mean(f1_score), np.std(f1_score)))\n",
    "        print('FE Time: {} seconds'.format(fe_time - start_time))\n",
    "        print('CL time: {} seconds'.format(cl_time - fe_time))\n",
    "        \n",
    "        f1_list.append(np.mean(f1_score))\n",
    "        acc_list.append(np.mean(acc_score))\n",
    "        count += 1\n",
    "    except Exception as e:\n",
    "        count += 1\n",
    "        fail += 1\n",
    "        print(i, \": \", e)\n",
    "        pass\n",
    "    print(\"================================================\")\n",
    "\n",
    "print(\"================================================\")\n",
    "print(\"Overall Accuracy = {} ({})\".format(sum(acc_list)/len(acc_list), np.std(acc_list)))\n",
    "print(\"Overall F1-Score = {} ({})\".format(sum(f1_list)/len(f1_list), np.std(f1_list)))\n",
    "print(\"Failed = {}/{}\".format(fail, len_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "fail = 0\n",
    "\n",
    "f1_list = []\n",
    "acc_list = []\n",
    "\n",
    "for i in range(range_len):\n",
    "    try:\n",
    "        print(\"Running {}/{} ({})\".format(count, len_clean, clean[count-1]))\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Feature Extraction\n",
    "        result_fe, label, feat_name = ortho_tf(clean[i])\n",
    "        fe_time = time.time()\n",
    "\n",
    "        # classification\n",
    "        acc_score, f1_score = random_forest(result_fe, label)\n",
    "        cl_time = time.time()\n",
    "\n",
    "        print('Accuracy: {} ({:.5f})'.format(np.mean(acc_score), np.std(acc_score)))\n",
    "        print('F1-Score: {} ({:.5f})'.format(np.mean(f1_score), np.std(f1_score)))\n",
    "        print('FE Time: {} seconds'.format(fe_time - start_time))\n",
    "        print('CL time: {} seconds'.format(cl_time - fe_time))\n",
    "        \n",
    "        f1_list.append(np.mean(f1_score))\n",
    "        acc_list.append(np.mean(acc_score))\n",
    "        count += 1\n",
    "    except Exception as e:\n",
    "        count += 1\n",
    "        fail += 1\n",
    "        print(i, \": \", e)\n",
    "        pass\n",
    "    print(\"================================================\")\n",
    "\n",
    "print(\"================================================\")\n",
    "print(\"Overall Accuracy = {} ({})\".format(sum(acc_list)/len(acc_list), np.std(acc_list)))\n",
    "print(\"Overall F1-Score = {} ({})\".format(sum(f1_list)/len(f1_list), np.std(f1_list)))\n",
    "print(\"Failed = {}/{}\".format(fail, len_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "fail = 0\n",
    "\n",
    "f1_list = []\n",
    "acc_list = []\n",
    "\n",
    "for i in range(range_len):\n",
    "    try:\n",
    "        print(\"Running {}/{} ({})\".format(count, len_clean, clean[count-1]))\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Feature Extraction\n",
    "        result_fe, label, feat_name = ortho_tf(clean[i])\n",
    "        fe_time = time.time()\n",
    "\n",
    "        # classification\n",
    "        acc_score, f1_score = svm(result_fe, label)\n",
    "        cl_time = time.time()\n",
    "\n",
    "        print('Accuracy: {} ({:.5f})'.format(np.mean(acc_score), np.std(acc_score)))\n",
    "        print('F1-Score: {} ({:.5f})'.format(np.mean(f1_score), np.std(f1_score)))\n",
    "        print('FE Time: {} seconds'.format(fe_time - start_time))\n",
    "        print('CL time: {} seconds'.format(cl_time - fe_time))\n",
    "        \n",
    "        f1_list.append(np.mean(f1_score))\n",
    "        acc_list.append(np.mean(acc_score))\n",
    "        count += 1\n",
    "    except Exception as e:\n",
    "        count += 1\n",
    "        fail += 1\n",
    "        print(i, \": \", e)\n",
    "        pass\n",
    "    print(\"================================================\")\n",
    "\n",
    "print(\"================================================\")\n",
    "print(\"Overall Accuracy = {} ({})\".format(sum(acc_list)/len(acc_list), np.std(acc_list)))\n",
    "print(\"Overall F1-Score = {} ({})\".format(sum(f1_list)/len(f1_list), np.std(f1_list)))\n",
    "print(\"Failed = {}/{}\".format(fail, len_clean))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
