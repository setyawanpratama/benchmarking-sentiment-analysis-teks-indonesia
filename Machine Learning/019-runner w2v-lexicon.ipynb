{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import time\n",
    "import warnings\n",
    "import datetime\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from featureExtraction.lexicon.lexicon import *\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = [x for x in os.listdir(\"../Dataset/Clean\") if (x.lower().endswith(\".csv\"))]\n",
    "clean.sort()\n",
    "\n",
    "DATA_CLEAN = [\"../Dataset/Clean/\" + x for x in os.listdir(\"../Dataset/Clean\") if (x.lower().endswith(\".csv\"))]\n",
    "DATA_CLEAN.sort()\n",
    "len_clean = len(DATA_CLEAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_sg(filepath, hs, neg, vector_size):\n",
    "    df = pd.read_csv(filepath, delimiter=\";\", low_memory=False, header=0)\n",
    "    df.dropna(axis=0, inplace=True)\n",
    "    model_path = './featureExtraction/word2vec/models_all/model_sg_{}_{}.model'.format('hs' if hs == 1 else 'neg', vector_size)\n",
    "    model_sg = Word2Vec.load(model_path)\n",
    "\n",
    "    word2vec_arr=[]\n",
    "    for row in df['teks'].tolist():\n",
    "        tweets = row.split(\" \")\n",
    "        row_mean_vector = (np.mean([model_sg.wv[terms] for terms in tweets], axis=0)).tolist()\n",
    "        if not (type(row_mean_vector) is list):\n",
    "            row_mean_vector = [float(0) for i in range(vector_size)]\n",
    "        word2vec_arr.append(row_mean_vector)\n",
    "\n",
    "    return np.array(word2vec_arr), df['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "count = 1\n",
    "fail = 0\n",
    "\n",
    "f1_list = []\n",
    "acc_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 1/13 (templated_posneg_Dataset_C_HT_4000.csv)\n",
      "Accuracy: 0.9540000000000001 (0.01420)\n",
      "F1-Score: 0.9538081029701729 (0.01425)\n",
      "FE Time: 3.7925074100494385 seconds\n",
      "CL time: 5.2090229988098145 seconds\n",
      "================================================\n",
      "Running 2/13 (templated_posneg_KicauanBerlabel.csv)\n",
      "Accuracy: 0.8709999999999999 (0.07715)\n",
      "F1-Score: 0.8613478610716788 (0.08249)\n",
      "FE Time: 0.8371052742004395 seconds\n",
      "CL time: 1.6382126808166504 seconds\n",
      "================================================\n",
      "Running 3/13 (templated_posneg_dataset_komentar_instagram_cyberbullying.csv)\n",
      "Accuracy: 0.8667105263157895 (0.07777)\n",
      "F1-Score: 0.8630364691469821 (0.08130)\n",
      "FE Time: 0.8439080715179443 seconds\n",
      "CL time: 0.20022964477539062 seconds\n",
      "================================================\n",
      "Running 4/13 (templated_posneg_dataset_tweet_sentimen_tayangan_tv.csv)\n",
      "Accuracy: 0.865 (0.05937)\n",
      "F1-Score: 0.8572206795108096 (0.06353)\n",
      "FE Time: 0.44586825370788574 seconds\n",
      "CL time: 0.1783764362335205 seconds\n",
      "================================================\n",
      "Running 5/13 (templated_posneg_dataset_tweet_sentiment_cellular_service_provider.csv)\n",
      "Accuracy: 0.786904761904762 (0.11729)\n",
      "F1-Score: 0.7679301203873824 (0.12068)\n",
      "FE Time: 0.3157176971435547 seconds\n",
      "CL time: 0.1398310661315918 seconds\n",
      "================================================\n",
      "Running 6/13 (templated_posneg_dataset_tweet_sentiment_opini_film.csv)\n",
      "Accuracy: 0.8138888888888889 (0.13165)\n",
      "F1-Score: 0.7949444999444999 (0.14691)\n",
      "FE Time: 0.36211585998535156 seconds\n",
      "CL time: 0.15730643272399902 seconds\n",
      "================================================\n",
      "Running 7/13 (templated_posneg_dataset_tweet_sentiment_pilkada_DKI_2017.csv)\n",
      "Accuracy: 0.7526515151515151 (0.07489)\n",
      "F1-Score: 0.7495967784013194 (0.07491)\n",
      "FE Time: 1.068666934967041 seconds\n",
      "CL time: 0.6026091575622559 seconds\n",
      "================================================\n",
      "Running 8/13 (templated_posneg_hotel.csv)\n",
      "Accuracy: 0.8900456913827653 (0.00821)\n",
      "F1-Score: 0.8898903707772512 (0.00832)\n",
      "FE Time: 18.326697826385498 seconds\n",
      "CL time: 20.038790464401245 seconds\n",
      "================================================\n",
      "Running 9/13 (templated_posneg_id-apps-review-sentimentanalysis.csv)\n",
      "Accuracy: 0.875 (0.14098)\n",
      "F1-Score: 0.858394314019314 (0.15305)\n",
      "FE Time: 0.31680774688720703 seconds\n",
      "CL time: 0.12496805191040039 seconds\n",
      "================================================\n",
      "Running 10/13 (templated_posneg_id-movie-review-sentimentanalysis.csv)\n",
      "Accuracy: 0.69 (0.07681)\n",
      "F1-Score: 0.68512581315678 (0.07631)\n",
      "FE Time: 0.9235880374908447 seconds\n",
      "CL time: 0.45381855964660645 seconds\n",
      "================================================\n",
      "Running 11/13 (templated_posneg_raw_data_sentiment2.csv)\n",
      "Accuracy: 0.7696099628124189 (0.01694)\n",
      "F1-Score: 0.7649368762881225 (0.01762)\n",
      "FE Time: 10.953034162521362 seconds\n",
      "CL time: 12.363770961761475 seconds\n",
      "================================================\n",
      "Running 12/13 (templated_posnegnet_tweetcleannew600-only.csv)\n",
      "Accuracy: 0.6132478632478632 (0.04418)\n",
      "F1-Score: 0.6093854789927182 (0.04347)\n",
      "FE Time: 3.285581350326538 seconds\n",
      "CL time: 6.306084156036377 seconds\n",
      "================================================\n",
      "Running 13/13 (templated_posnegnet_tweets_tagged_preprocessed.csv)\n",
      "Accuracy: 0.776 (0.07464)\n",
      "F1-Score: 0.6230445274797725 (0.13880)\n",
      "FE Time: 0.6824426651000977 seconds\n",
      "CL time: 1.2252535820007324 seconds\n",
      "================================================\n",
      "================================================\n",
      "Overall Accuracy = 0.8095430161310773 (0.0881710759625214)\n",
      "Overall F1-Score = 0.790666299395908 (0.10026621972335689)\n",
      "Failed = 0/13\n"
     ]
    }
   ],
   "source": [
    "for i in DATA_CLEAN:\n",
    "    try:\n",
    "        print(\"Running {}/{} ({})\".format(count, len_clean, clean[count-1]))\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Feature Extraction\n",
    "        result_fe1, label = word2vec_sg(i, 1, 0, 300)\n",
    "        result_fe2, label, feat_name = run_lexiconCombined_tweet(i)\n",
    "        result_fe = np.concatenate((result_fe1, result_fe2), axis=1)\n",
    "        fe_time = time.time()\n",
    "\n",
    "        # prepare classifier\n",
    "        cv = KFold(n_splits=20, random_state=1, shuffle=True)\n",
    "        logRes = LogisticRegression(random_state=1, max_iter=10000)\n",
    "        \n",
    "        # prepare the cross-validation procedure\n",
    "        acc_score = cross_val_score(logRes, result_fe, label, scoring='f1_micro', cv=cv, n_jobs=-1)\n",
    "        f1_score = cross_val_score(logRes, result_fe, label, scoring='f1_macro', cv=cv, n_jobs=-1)\n",
    "        cl_time = time.time()\n",
    "\n",
    "        print('Accuracy: {} ({:.5f})'.format(np.mean(acc_score), np.std(acc_score)))\n",
    "        print('F1-Score: {} ({:.5f})'.format(np.mean(f1_score), np.std(f1_score)))\n",
    "        print('FE Time: {} seconds'.format(fe_time - start_time))\n",
    "        print('CL time: {} seconds'.format(cl_time - fe_time))\n",
    "        \n",
    "        f1_list.append(np.mean(f1_score))\n",
    "        acc_list.append(np.mean(acc_score))\n",
    "        count += 1\n",
    "    except Exception as e:\n",
    "        count += 1\n",
    "        fail += 1\n",
    "        print(i, \": \", e)\n",
    "        pass\n",
    "    print(\"================================================\")\n",
    "\n",
    "print(\"================================================\")\n",
    "print(\"Overall Accuracy = {} ({})\".format(sum(acc_list)/len(acc_list), np.std(acc_list)))\n",
    "print(\"Overall F1-Score = {} ({})\".format(sum(f1_list)/len(f1_list), np.std(f1_list)))\n",
    "print(\"Failed = {}/{}\".format(fail, len_clean))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
